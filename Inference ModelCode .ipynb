{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Please change the runtime to GPU T4 to run this notebook"
      ],
      "metadata": {
        "id": "1VyC9agZvygg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "00ujIjK1tQ98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9hwP_9V4V7V"
      },
      "outputs": [],
      "source": [
        "#@title Install and import the necessary libraries\n",
        "!pip install torch\n",
        "!pip install -q -U accelerate peft bitsandbytes transformers trl einops datasets requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Importing transformer, Trainer and other necessary libraries\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from datasets import load_from_disk\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "S-Cb8fcD4mmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Defining base model, dataset and tokenizer\n",
        "\n",
        "# Model\n",
        "base_model = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "new_model = \"phi-3-clinical\"\n",
        "\n",
        "# Dataset\n",
        "dataset = load_dataset(\"hackint0sh/small-data\", split=\"train\")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "tokenizer.padding_side=\"right\""
      ],
      "metadata": {
        "id": "vusgHhLh4oUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Defining Quantization configuration, loading base model and setting trainer arguments\n",
        "\n",
        "# Quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "\n",
        "# Set training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir = \"./results\",\n",
        "    num_train_epochs = 1,\n",
        "    fp16 = True,\n",
        "    bf16 = False,\n",
        "    per_device_train_batch_size = 6,\n",
        "    per_device_eval_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1,\n",
        "    gradient_checkpointing = True,\n",
        "    max_grad_norm = 0.3,\n",
        "    learning_rate = 2e-4,\n",
        "    weight_decay = 0.001,\n",
        "    optim = \"paged_adamw_32bit\",\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    max_steps = -1,\n",
        "    warmup_ratio = 0.03,\n",
        "    group_by_length = True,\n",
        "    save_steps = 0,\n",
        "    logging_steps = 25,\n",
        ")"
      ],
      "metadata": {
        "id": "mUMsusgB4pj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setting Trainer with some hyperparameter\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=64,                  # Rank of the low-rank matrices\n",
        "    lora_alpha= 16,        # Scaling factor for the low-rank matrices\n",
        "    lora_dropout=0.05,     # Dropout applied to LoRA modifications\n",
        "    bias=\"none\",           # Bias configuration in LoRA\n",
        "    task_type=\"CAUSAL_LM\", # Specifies the task type\n",
        "    target_modules=[\"qkv_proj\", \"o_proj\"]  # Corrected target modules based on the actual model structure\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"data\",  # Adjust this field name based on your JSON structure\n",
        "    max_seq_length=1200,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n"
      ],
      "metadata": {
        "id": "aCBGoJ1D4tLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training model\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "EJA5ggQ74uty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Testing the model\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def generate_response(prompt, min_length=50, max_length=200):\n",
        "    # Generate text with parameters set to ensure completeness and control over the generation process.\n",
        "    generated_results = generator(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=False,\n",
        "        temperature=0.9,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return generated_results[0]['generated_text']\n",
        "\n",
        "# Prompt the model\n",
        "prompt = None #@param\n",
        "response = generate_response(prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "im9ukYTDJD0H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}